# Gu√≠a Completa de llama.cpp - Tu IA Local en Terminal

> Una gu√≠a pr√°ctica para dominar el ecosistema de modelos de IA locales

---

## üß† Conceptos Fundamentales (Para empezar bien)

### ¬øQu√© es un archivo `.gguf`?
Es como un **archivo ZIP pero para modelos de IA**. Contiene todo lo necesario:
- La arquitectura del modelo (c√≥mo est√° construido)
- Los pesos neuronales (el "conocimiento" entrenado)  
- El tokenizador (c√≥mo convierte texto en n√∫meros)
- Metadatos y configuraci√≥n

**Por eso son port√°tiles**: copias el archivo y ya tienes la IA completa.

### ¬øQu√© es la Cuantizaci√≥n?
Imagina una foto en 4K vs la misma foto comprimida en JPEG:
- **F16**: Precisi√≥n m√°xima, consumo de RAM brutal
- **Q8_0**: Casi sin p√©rdida, 50% menos RAM  
- **Q6_K**: Excelente equilibrio calidad/tama√±o
- **Q5_K_M**: Buena calidad, compacto
- **Q4_K_M**: Est√°ndar recomendado para la mayor√≠a
- **Q4_K_S**: Muy compacto, p√©rdida notable pero funcional

### Tokens: La "Moneda" de la IA
Los modelos no leen palabras, leen **tokens**:
- `"Hola"` = 1 token
- `"artificial"` = 2 tokens (`"art"` + `"ificial"`)
- `"ü§ñ"` = 1 token

**Regla pr√°ctica**: 1 token ‚âà 0.75 palabras en espa√±ol.

---

## üéØ Tipos de Modelos y Sus Especialidades

### Por Idioma y Dominio

| Especialidad | Idioma Principal | Estilo | Casos de Uso T√≠picos |
|--------------|-----------------|--------|---------------------|
| **T√©cnico/Alem√°n** | Alem√°n | Formal, normativo | Documentos oficiales, traducciones precisas |
| **Programaci√≥n** | Ingl√©s | An√°lisis estructurado | Debugging, arquitectura, code review |
| **Japon√©s** | Japon√©s | Imperativos, instrucciones | Documentaci√≥n t√©cnica japonesa |
| **Contexto Largo** | Multiling√ºe | Conocimiento amplio | An√°lisis complejos, investigaci√≥n |
| **General/Balanceado** | Ingl√©s | Generaci√≥n fluida | Uso diario, tareas variadas |
| **M√©dico/Cl√≠nico** | Ingl√©s | Cl√≠nico, biom√©dico | Textos m√©dicos, farmac√©uticos |
| **Conversacional** | Ingl√©s | Di√°logo natural | Chat, atenci√≥n al cliente |
| **Documentaci√≥n** | Ingl√©s | Explicaciones suaves | Manuales, gu√≠as t√©cnicas |
| **Compacto/R√°pido** | Ingl√©s | Razonamiento eficiente | Pruebas r√°pidas, hardware limitado |
| **Multiidioma** | Chino/Ingl√©s | Contexto extenso | Documentos internacionales |
| **Ultra Ligero** | Ingl√©s | Testing r√°pido | Desarrollo, IoT, experimentos |
| **Python Especializado** | Ingl√©s | Tutoriales detallados | Ense√±anza de programaci√≥n |
| **Acad√©mico** | Chino/Ingl√©s | Papers cient√≠ficos | Investigaci√≥n, textos t√©cnicos |
| **Jur√≠dico/Formal** | Ingl√©s | Institucional | Contratos, pol√≠ticas, compliance |
| **Automatizaci√≥n** | Ingl√©s | Decisiones complejas | Workflows, gesti√≥n de procesos |
| **Base Sin Filtros** | Ingl√©s | Neutral | Experimentos, respuestas directas |
| **Narrativo** | Ingl√©s | Mitol√≥gico, storytelling | Worldbuilding, ficci√≥n √©pica |
| **Creativo Expresivo** | Ingl√©s | Dram√°tico, emotivo | Ficci√≥n, roleplay creativo |
| **Filos√≥fico** | Ingl√©s | Di√°logo socr√°tico | Debates, pensamiento cr√≠tico |
| **Sin Censura** | Ingl√©s | Temas sensibles | Investigaci√≥n de seguridad |
| **Roleplay Avanzado** | Ingl√©s | Narrativa libre | Roleplay, exploraci√≥n creativa |

---

## üõ† Herramientas del Ecosistema

### 1. N√∫cleo de Ejecuci√≥n

| Binario | Funci√≥n Principal | Cu√°ndo Usar |
|---------|-------------------|-------------|
| `llama-cli` | **Motor principal**. Ejecuta modelos desde terminal | Scripts, automatizaci√≥n, pruebas r√°pidas |
| `llama-run` | Chat interactivo con memoria de conversaci√≥n | Experimentar, dialogar con modelos |
| `llama-server` | Servidor web con API REST | Integrar con aplicaciones, uso remoto |

### 2. Herramientas de An√°lisis

| Herramienta | Funci√≥n | Utilidad Pr√°ctica |
|-------------|---------|-------------------|
| `llama-tokenize` | Muestra c√≥mo el modelo interpreta tu texto | Optimizar prompts, entender l√≠mites |
| `llama-bench` | Mide rendimiento en tu hardware | Comparar modelos, optimizar configuraci√≥n |
| `llama-embedding` | Convierte texto en vectores num√©ricos | B√∫squeda sem√°ntica, an√°lisis de similitud |

### 3. Herramientas de Optimizaci√≥n

| Herramienta | Prop√≥sito | Cu√°ndo es Necesaria |
|-------------|-----------|-------------------|
| `llama-quantize` | Comprime modelos para menos RAM | Tu hardware no soporta el modelo completo |
| `llama-gguf-split` | Divide modelos en fragmentos | Descargas lentas, almacenamiento limitado |
| `llama-gguf-hash` | Verifica integridad de archivos | Asegurar descargas correctas |

---

## ‚öôÔ∏è Par√°metros Esenciales

### B√°sicos (Imprescindibles)

| Par√°metro | Funci√≥n | Valores T√≠picos | Ejemplo Pr√°ctico |
|-----------|---------|----------------|------------------|
| `-m` | Ruta al modelo | Ruta absoluta | `-m ~/modelos/mistral-7b.gguf` |
| `-p` | Tu prompt/pregunta | Texto libre | `-p "Explica la fotos√≠ntesis"` |
| `-n, --n-predict` | Tokens m√°ximos a generar | 128-2048 | `-n 512` (respuesta media) |
| `-c, --ctx-size` | Tama√±o del contexto | 512-16384 | `-c 4096` (documento largo) |

### Control de Contexto y Memoria

| Configuraci√≥n | Uso de RAM Aprox. | Escenario Ideal |
|---------------|-------------------|-----------------|
| `--ctx-size 1024` | ~1-2MB | Chat b√°sico, preguntas cortas |
| `--ctx-size 2048` | ~2-4MB | Conversaciones normales |
| `--ctx-size 4096` | ~4-8MB | Documentos medianos, an√°lisis |
| `--ctx-size 8192` | ~8-16MB | Textos largos, investigaci√≥n |
| `--ctx-size 16384` | ~16-32MB | Documentos muy extensos |

### Control de Creatividad

| Temperatura | Comportamiento | Casos de Uso |
|-------------|----------------|--------------|
| `--temp 0.1` | **Robot**: Muy determinista | C√≥digo, correcciones, datos precisos |
| `--temp 0.3` | **T√©cnico**: Preciso pero flexible | Documentaci√≥n, explicaciones |
| `--temp 0.7` | **Humano**: Equilibrio natural | Conversaci√≥n general |
| `--temp 0.9` | **Creativo**: Din√°mico | Brainstorming, ideas |
| `--temp 1.2` | **Artista**: Muy experimental | Ficci√≥n, narrativa libre |

### Control de Calidad de Salida

| Par√°metro | Efecto | Valor Conservador | Valor Creativo |
|-----------|--------|------------------|----------------|
| `--top-p` | Variedad de vocabulario | 0.9 | 0.95 |
| `--top-k` | L√≠mite de opciones | 20-40 | 80-100 |
| `--repeat-penalty` | Previene repeticiones | 1.1 | 1.05 |
| `--repeat-last-n` | Ventana anti-repetici√≥n | 64 | 128 |

### Optimizaci√≥n de Rendimiento

| Par√°metro | Funci√≥n | Configuraci√≥n T√≠pica |
|-----------|---------|---------------------|
| `-t, --threads` | Hilos de CPU | N√∫mero de cores disponibles |
| `--batch-size` | Procesamiento por lotes | 512-2048 (seg√∫n RAM) |
| `--gpu-layers` | Capas en GPU | 99 (usar toda la GPU disponible) |

---

## üìã Recetas por Especialidad

### üîß Correcci√≥n y Edici√≥n de Texto

```bash
# Correcci√≥n precisa de documentos
./llama-cli \
    -m ./modelos/mistral-7b-instruct.gguf \
    -p "Corrige errores ortogr√°ficos y gramaticales: $(cat documento.txt)" \
    -c 4096 \
    -n 512 \
    --temp 0.2 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    --silent
```

### üíª An√°lisis y Revisi√≥n de C√≥digo

```bash
# Revisi√≥n de c√≥digo con an√°lisis detallado
./llama-cli \
    -m ./modelos/deepseek-coder.gguf \
    -p "Analiza este c√≥digo y sugiere mejoras: $(cat script.py)" \
    -c 8192 \
    -n 1024 \
    --temp 0.1 \
    --repeat-penalty 1.1 \
    --silent
```

### üé® Generaci√≥n Creativa

```bash
# Escritura creativa con alta expresividad
./llama-cli \
    -m ./modelos/chronos-hermes.gguf \
    -p "Escribe una historia √©pica sobre el despertar de una IA" \
    -c 4096 \
    -n 1500 \
    --temp 0.9 \
    --top-p 0.95 \
    --repeat-penalty 1.05
```

### üîç An√°lisis de Documentos Extensos

```bash
# Procesamiento de contexto muy largo
./llama-cli \
    -m ./modelos/llama-70b.gguf \
    -p "Resume y analiza este documento completo: $(cat documento_largo.txt)" \
    -c 16384 \
    -n 2048 \
    --temp 0.5 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    -t 8
```

### üí¨ Conversaci√≥n Natural

```bash
# Chat interactivo con memoria
./llama-run \
    -m ./modelos/openchat.gguf \
    --repeat-penalty 1.1 \
    --temp 0.7 \
    -c 2048 \
    -i
```

---

## üß™ Estrategias por Tipo de Tarea

### Tareas T√©cnicas y Factuales
```bash
# Configuraci√≥n para precisi√≥n m√°xima
--temp 0.1-0.3 --top-p 0.9 --repeat-penalty 1.1
# Modelos recomendados: Coder, Medical, Technical
```

### Conversaci√≥n y Explicaciones
```bash
# Configuraci√≥n balanceada y natural
--temp 0.6-0.8 --top-p 0.9 --repeat-penalty 1.1 --repeat-last-n 64
# Modelos recomendados: Chat, General-purpose, Instruction-following
```

### Creatividad y Brainstorming
```bash
# Configuraci√≥n para m√°xima expresividad
--temp 0.8-1.2 --top-p 0.95 --repeat-penalty 1.05
# Modelos recomendados: Creative, Storytelling, Roleplay
```

### Investigaci√≥n y An√°lisis
```bash
# Configuraci√≥n para profundidad anal√≠tica
--temp 0.3-0.5 --top-p 0.9 -c 8192+ --n-predict 1024+
# Modelos recomendados: Large context, Academic, Research-focused
```

---

## üöÄ Automatizaci√≥n e Integraci√≥n

### Script de Selecci√≥n Autom√°tica de Modelo

```bash
#!/bin/bash
# Selector inteligente basado en tipo de tarea

select_model_by_task() {
    local task="$1"
    local base_path="./modelos"
    
    case "$task" in
        "code"|"programming")
            echo "$base_path/deepseek-coder.gguf"
            ;;
        "creative"|"story")
            echo "$base_path/chronos-hermes.gguf"
            ;;
        "medical"|"health")
            echo "$base_path/meditron.gguf"
            ;;
        "legal"|"formal")
            echo "$base_path/nous-hermes-legal.gguf"
            ;;
        "research"|"academic")
            echo "$base_path/llama-70b.gguf"
            ;;
        *)
            echo "$base_path/mistral-instruct.gguf"
            ;;
    esac
}

# Uso del selector
TASK_TYPE="$1"
MODELO=$(select_model_by_task "$TASK_TYPE")
PROMPT="$2"

./llama-cli -m "$MODELO" -p "$PROMPT" -c 4096 -n 512 --temp 0.7
```

### Servidor Multi-Modelo

```bash
#!/bin/bash
# Lanzar m√∫ltiples modelos como servicios

start_model_server() {
    local model_path="$1"
    local port="$2"
    local model_name="$3"
    
    ./llama-server \
        -m "$model_path" \
        --host 0.0.0.0 \
        --port "$port" \
        -c 4096 \
        --gpu-layers 99 &
    
    echo "‚úÖ $model_name servidor iniciado en puerto $port"
}

# Iniciar servicios especializados
start_model_server "./modelos/mistral-general.gguf" 8080 "General"
start_model_server "./modelos/deepseek-coder.gguf" 8081 "C√≥digo"
start_model_server "./modelos/creative-model.gguf" 8082 "Creativo"

echo "üåê Servidores disponibles:"
echo "  General: http://localhost:8080"
echo "  C√≥digo: http://localhost:8081" 
echo "  Creativo: http://localhost:8082"
```

### Pipeline de Procesamiento de Documentos

```bash
#!/bin/bash
# Pipeline completo: OCR ‚Üí Correcci√≥n ‚Üí An√°lisis

process_document() {
    local input_image="$1"
    local output_dir="./processed"
    
    mkdir -p "$output_dir"
    
    # 1. OCR del documento
    tesseract "$input_image" "$output_dir/raw_text"
    
    # 2. Correcci√≥n con IA
    ./llama-cli \
        -m ./modelos/correction-model.gguf \
        -p "Corrige errores en este texto: $(cat "$output_dir/raw_text.txt")" \
        -c 4096 -n 1024 --temp 0.2 --silent \
        > "$output_dir/corrected_text.txt"
    
    # 3. An√°lisis y resumen
    ./llama-cli \
        -m ./modelos/analysis-model.gguf \
        -p "Resume los puntos clave: $(cat "$output_dir/corrected_text.txt")" \
        -c 2048 -n 256 --temp 0.5 --silent \
        > "$output_dir/summary.txt"
    
    echo "‚úÖ Documento procesado en $output_dir"
}

# Usar el pipeline
process_document "documento_escaneado.png"
```

---

## üõ° Soluci√≥n de Problemas Comunes

### Modelos Muy Grandes (70B+)
**S√≠ntoma**: Sistema lento o sin memoria suficiente
**Soluciones**:
```bash
# Reducir uso de memoria
-c 2048              # Menos contexto
-t 4                 # Menos hilos
--gpu-layers 50      # Solo parte en GPU
# O usar cuantizaci√≥n m√°s agresiva (Q4_K_S)
```

### Modelos de C√≥digo
**S√≠ntoma**: Respuestas incompletas o c√≥digo cortado
**Soluciones**:
```bash
-n 2048              # M√°s tokens de salida
--temp 0.1           # M√°xima precisi√≥n
-c 8192              # M√°s contexto para c√≥digo largo
--ignore-eos         # No cortar prematuramente
```

### Modelos Creativos
**S√≠ntoma**: Repetici√≥n o p√©rdida de coherencia
**Soluciones**:
```bash
--repeat-penalty 1.1  # Penalizar repeticiones
--mirostat 2          # Control autom√°tico
--temp 0.8            # No exceder temperatura
-c 4096+              # M√°s contexto para coherencia
```

### Detecci√≥n de Problemas de Rendimiento
```bash
# Monitorear uso de recursos
watch -n 1 'ps aux | grep llama-cli'

# Benchmark r√°pido
./llama-bench -m modelo.gguf -p 512 -n 128

# Test de memoria
./llama-cli -m modelo.gguf -c 1024 -n 10 --temp 0.1 -p "Test"
```

---

## üéØ Configuraci√≥n Productiva

### Variables de Entorno √ötiles

```bash
# A√±adir a tu .bashrc o .zshrc
export LLAMA_HOME="./llama.cpp/build/bin"
export MODELS_DIR="./modelos"

# Aliases para uso r√°pido
alias llama='$LLAMA_HOME/llama-cli -m $MODELS_DIR/general-model.gguf'
alias llama-code='$LLAMA_HOME/llama-cli -m $MODELS_DIR/code-model.gguf'
alias llama-creative='$LLAMA_HOME/llama-cli -m $MODELS_DIR/creative-model.gguf'

# Configuraciones predefinidas
alias quick-fix='llama -n 256 --temp 0.2 --repeat-penalty 1.1 --silent -p'
alias code-review='llama-code -c 8192 -n 1024 --temp 0.1 --silent -p'
alias brainstorm='llama-creative -c 4096 -n 800 --temp 0.9 --top-p 0.95 -p'
```

### Script de Benchmark Completo

```bash
#!/bin/bash
# Evaluar el rendimiento de todos tus modelos

benchmark_all() {
    local models_dir="$1"
    
    echo "üìä BENCHMARK DE MODELOS"
    echo "======================"
    
    for model in "$models_dir"/*.gguf; do
        model_name=$(basename "$model" .gguf)
        echo "üß™ Evaluando: $model_name"
        
        ./llama-bench \
            -m "$model" \
            -p 512 \
            -n 128 \
            -t $(nproc) 2>/dev/null | \
            grep "llama_print_timings" || echo "‚ùå Error en $model_name"
        echo ""
    done
    
    echo "‚úÖ Benchmark completado"
}

# Ejecutar benchmark
benchmark_all "./modelos"
```

---

## üìö Referencias T√©cnicas

### Formatos de Cuantizaci√≥n (Ordenados por Calidad/Tama√±o)

| Formato | Calidad | Tama√±o | Recomendado Para |
|---------|---------|--------|------------------|
| **Q8_0** | 99% | 50% del original | M√°xima calidad, hardware potente |
| **Q6_K** | 98% | 60% del original | Equilibrio excelente |
| **Q5_K_M** | 95% | 70% del original | Uso general recomendado |
| **Q4_K_M** | 90% | 50% del original | Hardware limitado |
| **Q4_K_S** | 85% | 45% del original | M√°xima compresi√≥n √∫til |

### Comandos de Diagn√≥stico

```bash
# Verificar integridad de modelo
./llama-gguf-hash -f modelo.gguf

# An√°lisis de tokenizaci√≥n
./llama-tokenize -m modelo.gguf -p "Tu texto aqu√≠"

# Test r√°pido de funcionalidad
./llama-cli -m modelo.gguf -p "2+2=" -n 5 --temp 0.1

# Informaci√≥n del modelo
./llama-cli -m modelo.gguf --help | head -20
```

### Pr√≥ximos Pasos Recomendados

1. **Experimenta con temperaturas** para diferentes tipos de tareas
2. **Configura aliases** para tus workflows m√°s comunes  
3. **Prueba el modo servidor** para integrar con otras aplicaciones
4. **Optimiza el contexto** seg√∫n el tipo de documentos que procesas
5. **Automatiza la selecci√≥n** de modelos seg√∫n el contenido

---

> **Estructura t√≠pica**: `./llama.cpp/build/bin/` (binarios) y `./modelos/` (archivos .gguf)
> 
> **Instalaci√≥n**: Compilar llama.cpp desde el repositorio oficial de GitHub

¬°Tienes todo lo necesario para dominar tu ecosistema de IA local!
