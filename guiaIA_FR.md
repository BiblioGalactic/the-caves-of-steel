Guide complet de llama.cpp ‚Äî Votre IA locale dans le terminal

Un guide pratique pour ma√Ætriser l‚Äô√©cosyst√®me des mod√®les d‚ÄôIA locaux

‚∏ª

üß† Concepts fondamentaux (pour bien commencer)

Qu‚Äôest-ce qu‚Äôun fichier .gguf ?

C‚Äôest comme un fichier ZIP mais pour les mod√®les d‚ÄôIA. Il contient tout le n√©cessaire :
	‚Ä¢	L‚Äôarchitecture du mod√®le (comment il est construit)
	‚Ä¢	Les poids neuronaux (la ¬´ connaissance ¬ª entra√Æn√©e)
	‚Ä¢	Le tokenizer (comment il transforme le texte en nombres)
	‚Ä¢	Les m√©tadonn√©es et la configuration

C‚Äôest pourquoi ils sont portables : on copie le fichier et on a l‚ÄôIA compl√®te.

Qu‚Äôest-ce que la quantification ?

Imaginez une photo en 4K vs la m√™me photo compress√©e en JPEG :
	‚Ä¢	F16 : Pr√©cision maximale, consommation de RAM √©lev√©e
	‚Ä¢	Q8_0 : Presque sans perte, ~50 % de RAM en moins
	‚Ä¢	Q6_K : Excellent √©quilibre qualit√©/taille
	‚Ä¢	Q5_K_M : Bonne qualit√©, compact
	‚Ä¢	Q4_K_M : Standard recommand√© pour la plupart des usages
	‚Ä¢	Q4_K_S : Tr√®s compact, perte notable mais utilisable

Tokens : la ¬´ monnaie ¬ª de l‚ÄôIA

Les mod√®les ne lisent pas des mots, ils lisent des tokens :
	‚Ä¢	"Hola" = 1 token
	‚Ä¢	"artificial" = 2 tokens ("art" + "ificial")
	‚Ä¢	"ü§ñ" = 1 token

R√®gle pratique : 1 token ‚âà 0,75 mot en espagnol/fran√ßais.

‚∏ª

üéØ Types de mod√®les et leurs sp√©cialit√©s

Par langue et domaine

Sp√©cialit√©	Langue principale	Style	Cas d‚Äôusage typiques
Technique / Allemand	Allemand	Formel, normatif	Documents officiels, traductions pr√©cises
Programmation	Anglais	Analyse structur√©e	Debugging, architecture, revue de code
Japonais	Japonais	Imp√©ratifs, instructions	Documentation technique japonaise
Contexte long	Multilingue	Connaissance large	Analyses complexes, recherche
G√©n√©ral / √âquilibr√©	Anglais	G√©n√©ration fluide	Usage quotidien, t√¢ches vari√©es
M√©dical / Clinique	Anglais	Clinique, biom√©dical	Textes m√©dicaux, pharmaceutiques
Conversationnel	Anglais	Dialogue naturel	Chat, support client
Documentation	Anglais	Explications structur√©es	Manuels, guides techniques
Compact / Rapide	Anglais	Raisonnement efficace	Tests rapides, hardware limit√©
Multi-langue	Chinois / Anglais	Contexte √©tendu	Documents internationaux
Ultra l√©ger	Anglais	Tests rapides	D√©veloppement, IoT, exp√©riences
Python sp√©cialis√©	Anglais	Tutoriels d√©taill√©s	Enseignement de la programmation
Acad√©mique	Chinois / Anglais	Articles scientifiques	Recherche, textes techniques
Juridique / Formel	Anglais	Institutionnel	Contrats, politiques, conformit√©
Automatisation	Anglais	D√©cisions complexes	Workflows, gestion de processus
Base sans filtres	Anglais	Neutre	Exp√©riences, r√©ponses directes
Narratif	Anglais	Mythologique, storytelling	Worldbuilding, fiction √©pique
Cr√©atif expressif	Anglais	Dramatique, √©motionnel	Fiction, roleplay cr√©atif
Philosophique	Anglais	Dialogue socratique	D√©bats, pens√©e critique
Sans censure	Anglais	Sujets sensibles	Recherche s√©curit√©
Roleplay avanc√©	Anglais	Narration libre	Roleplay, exploration cr√©ative


‚∏ª

üõ† Outils de l‚Äô√©cosyst√®me

1. Noyau d‚Äôex√©cution

Binaire	Fonction principale	Quand l‚Äôutiliser
llama-cli	Moteur principal. Ex√©cute les mod√®les depuis le terminal	Scripts, automatisation, tests rapides
llama-run	Chat interactif avec m√©moire de conversation	Exp√©rimenter, dialoguer avec des mod√®les
llama-server	Serveur web avec API REST	Int√©gration applicative, usage √† distance

2. Outils d‚Äôanalyse

Outil	Fonction	Utilit√© pratique
llama-tokenize	Montre comment le mod√®le interpr√®te votre texte	Optimiser les prompts, comprendre les limites
llama-bench	Mesure les performances sur votre hardware	Comparer mod√®les, optimiser config
llama-embedding	Transforme du texte en vecteurs num√©riques	Recherche s√©mantique, similarit√©

3. Outils d‚Äôoptimisation

Outil	But	Quand l‚Äôutiliser
llama-quantize	Compresse les mod√®les pour r√©duire la RAM	Si votre hardware ne supporte pas le mod√®le complet
llama-gguf-split	Divise les mod√®les en fragments	T√©l√©chargements lents, stockage limit√©
llama-gguf-hash	V√©rifie l‚Äôint√©grit√© des fichiers	S‚Äôassurer de t√©l√©chargements corrects


‚∏ª

‚öôÔ∏è Param√®tres essentiels

Basique (indispensable)

Param√®tre	Fonction	Valeurs typiques	Exemple pratique
-m	Chemin vers le mod√®le	Chemin absolu	-m ~/modelos/mistral-7b.gguf
-p	Votre prompt / question	Texte libre	-p "Explique la photosynth√®se"
-n, --n-predict	Tokens max √† g√©n√©rer	128‚Äì2048	-n 512 (r√©ponse moyenne)
-c, --ctx-size	Taille du contexte	512‚Äì16384	-c 4096 (document long)

Contr√¥le du contexte et m√©moire

Configuration	RAM approximative	Sc√©nario id√©al
--ctx-size 1024	~1‚Äì2 MB	Chat basique, questions courtes
--ctx-size 2048	~2‚Äì4 MB	Conversations normales
--ctx-size 4096	~4‚Äì8 MB	Documents moyens, analyses
--ctx-size 8192	~8‚Äì16 MB	Textes longs, recherche
--ctx-size 16384	~16‚Äì32 MB	Documents tr√®s volumineux

Contr√¥le de la cr√©ativit√©

Temp√©rature	Comportement	Cas d‚Äôusage
--temp 0.1	Robot : tr√®s d√©terministe	Code, corrections, donn√©es pr√©cises
--temp 0.3	Technique : pr√©cis mais flexible	Documentation, explications
--temp 0.7	Humain : √©quilibre naturel	Conversation g√©n√©rale
--temp 0.9	Cr√©atif : dynamique	Brainstorming, id√©es
--temp 1.2	Artiste : tr√®s exp√©rimental	Fiction, narration libre

Contr√¥le de la qualit√© de sortie

Param√®tre	Effet	Valeur conservatrice	Valeur cr√©ative
--top-p	Vari√©t√© lexicale	0.9	0.95
--top-k	Limite d‚Äôoptions	20‚Äì40	80‚Äì100
--repeat-penalty	Emp√™che les r√©p√©titions	1.1	1.05
--repeat-last-n	Fen√™tre anti-r√©p√©tition	64	128

Optimisation des performances

Param√®tre	Fonction	Configuration typique
-t, --threads	Threads CPU	Nombre de c≈ìurs disponibles
--batch-size	Traitement par lots	512‚Äì2048 (selon RAM)
--gpu-layers	Couches sur GPU	99 (utiliser toute la GPU disponible)


‚∏ª

üìã Recettes par sp√©cialit√©

üîß Correction et √©dition de texte

# Correcci√≥n precisa de documentos
./llama-cli \
    -m ./modelos/mistral-7b-instruct.gguf \
    -p "Corrige errores ortogr√°ficos y gramaticales: $(cat documento.txt)" \
    -c 4096 \
    -n 512 \
    --temp 0.2 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    --silent

Remarque : les blocs de code sont laiss√©s tels quels (commandes inchang√©es) pour rester imm√©diatement utilisables.

üíª Analyse et revue de code

# Revisi√≥n de c√≥digo con an√°lisis detallado
./llama-cli \
    -m ./modelos/deepseek-coder.gguf \
    -p "Analiza este c√≥digo y sugiere mejoras: $(cat script.py)" \
    -c 8192 \
    -n 1024 \
    --temp 0.1 \
    --repeat-penalty 1.1 \
    --silent

üé® G√©n√©ration cr√©ative

# Escritura creativa con alta expresividad
./llama-cli \
    -m ./modelos/chronos-hermes.gguf \
    -p "Escribe una historia √©pica sobre el despertar de una IA" \
    -c 4096 \
    -n 1500 \
    --temp 0.9 \
    --top-p 0.95 \
    --repeat-penalty 1.05

üîç Analyse de documents volumineux

# Procesamiento de contexto muy largo
./llama-cli \
    -m ./modelos/llama-70b.gguf \
    -p "Resume y analiza este documento completo: $(cat documento_largo.txt)" \
    -c 16384 \
    -n 2048 \
    --temp 0.5 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    -t 8

üí¨ Conversation naturelle

# Chat interactivo con memoria
./llama-run \
    -m ./modelos/openchat.gguf \
    --repeat-penalty 1.1 \
    --temp 0.7 \
    -c 2048 \
    -i


‚∏ª

üß™ Strat√©gies selon le type de t√¢che

T√¢ches techniques et factuelles

# Configuraci√≥n para precisi√≥n m√°xima
--temp 0.1-0.3 --top-p 0.9 --repeat-penalty 1.1
# Modelos recomendados: Coder, Medical, Technical

Conversation et explications

# Configuraci√≥n balanceada y natural
--temp 0.6-0.8 --top-p 0.9 --repeat-penalty 1.1 --repeat-last-n 64
# Modelos recomendados: Chat, General-purpose, Instruction-following

Cr√©ativit√© et brainstorming

# Configuraci√≥n para m√°xima expresividad
--temp 0.8-1.2 --top-p 0.95 --repeat-penalty 1.05
# Modelos recomendados: Creative, Storytelling, Roleplay

Recherche et analyse

# Configuraci√≥n para profundidad anal√≠tica
--temp 0.3-0.5 --top-p 0.9 -c 8192+ --n-predict 1024+
# Modelos recomendados: Large context, Academic, Research-focused


‚∏ª

üöÄ Automatisation et int√©gration

Script de s√©lection automatique de mod√®le

#!/bin/bash
# Selector inteligente basado en tipo de tarea

select_model_by_task() {
    local task="$1"
    local base_path="./modelos"
    
    case "$task" in
        "code"|"programming")
            echo "$base_path/deepseek-coder.gguf"
            ;;
        "creative"|"story")
            echo "$base_path/chronos-hermes.gguf"
            ;;
        "medical"|"health")
            echo "$base_path/meditron.gguf"
            ;;
        "legal"|"formal")
            echo "$base_path/nous-hermes-legal.gguf"
            ;;
        "research"|"academic")
            echo "$base_path/llama-70b.gguf"
            ;;
        *)
            echo "$base_path/mistral-instruct.gguf"
            ;;
    esac
}

# Uso del selector
TASK_TYPE="$1"
MODELO=$(select_model_by_task "$TASK_TYPE")
PROMPT="$2"

./llama-cli -m "$MODELO" -p "$PROMPT" -c 4096 -n 512 --temp 0.7

Serveur multi-mod√®les

#!/bin/bash
# Lanzar m√∫ltiples modelos como servicios

start_model_server() {
    local model_path="$1"
    local port="$2"
    local model_name="$3"
    
    ./llama-server \
        -m "$model_path" \
        --host 0.0.0.0 \
        --port "$port" \
        -c 4096 \
        --gpu-layers 99 &
    
    echo "‚úÖ $model_name servidor iniciado en puerto $port"
}

# Iniciar servicios especializados
start_model_server "./modelos/mistral-general.gguf" 8080 "General"
start_model_server "./modelos/deepseek-coder.gguf" 8081 "C√≥digo"
start_model_server "./modelos/creative-model.gguf" 8082 "Creativo"

echo "üåê Servidores disponibles:"
echo "  General: http://localhost:8080"
echo "  C√≥digo: http://localhost:8081" 
echo "  Creativo: http://localhost:8082"

Pipeline de traitement de documents

#!/bin/bash
# Pipeline completo: OCR ‚Üí Correcci√≥n ‚Üí An√°lisis

process_document() {
    local input_image="$1"
    local output_dir="./processed"
    
    mkdir -p "$output_dir"
    
    # 1. OCR del documento
    tesseract "$input_image" "$output_dir/raw_text"
    
    # 2. Correcci√≥n con IA
    ./llama-cli \
        -m ./modelos/correction-model.gguf \
        -p "Corrige errores en este texto: $(cat "$output_dir/raw_text.txt")" \
        -c 4096 -n 1024 --temp 0.2 --silent \
        > "$output_dir/corrected_text.txt"
    
    # 3. An√°lisis y resumen
    ./llama-cli \
        -m ./modelos/analysis-model.gguf \
        -p "Resume los puntos clave: $(cat "$output_dir/corrected_text.txt")" \
        -c 2048 -n 256 --temp 0.5 --silent \
        > "$output_dir/summary.txt"
    
    echo "‚úÖ Documento procesado en $output_dir"
}

# Usar el pipeline
process_document "documento_escaneado.png"


‚∏ª

üõ° R√©solution des probl√®mes courants

Mod√®les tr√®s grands (70B+)

Sympt√¥me : syst√®me lent ou manque de m√©moire
Solutions :

# R√©duire l‚Äôusage m√©moire
-c 2048              # Moins de contexte
-t 4                 # Moins de threads
--gpu-layers 50      # Seulement une partie sur GPU
# Ou utiliser une quantification plus agressive (Q4_K_S)

Mod√®les orient√©s code

Sympt√¥me : r√©ponses incompl√®tes ou code coup√©
Solutions :

-n 2048              # Plus de tokens de sortie
--temp 0.1           # Pr√©cision maximale
-c 8192              # Plus de contexte pour code long
--ignore-eos         # Ne pas couper pr√©matur√©ment

Mod√®les cr√©atifs

Sympt√¥me : r√©p√©titions ou perte de coh√©rence
Solutions :

--repeat-penalty 1.1  # P√©naliser les r√©p√©titions
--mirostat 2          # Contr√¥le automatique
--temp 0.8            # Ne pas d√©passer la temp√©rature
-c 4096+              # Plus de contexte pour la coh√©rence

D√©tection des probl√®mes de performance

# Surveiller l‚Äôutilisation des ressources
watch -n 1 'ps aux | grep llama-cli'

# Benchmark rapide
./llama-bench -m modelo.gguf -p 512 -n 128

# Test m√©moire
./llama-cli -m modelo.gguf -c 1024 -n 10 --temp 0.1 -p "Test"


‚∏ª

üéØ Configuration productive

Variables d‚Äôenvironnement utiles

# A√±adir a tu .bashrc o .zshrc
export LLAMA_HOME="./llama.cpp/build/bin"
export MODELS_DIR="./modelos"

# Aliases para uso r√°pido
alias llama='$LLAMA_HOME/llama-cli -m $MODELS_DIR/general-model.gguf'
alias llama-code='$LLAMA_HOME/llama-cli -m $MODELS_DIR/code-model.gguf'
alias llama-creative='$LLAMA_HOME/llama-cli -m $MODELS_DIR/creative-model.gguf'

# Configuraciones predefinidas
alias quick-fix='llama -n 256 --temp 0.2 --repeat-penalty 1.1 --silent -p'
alias code-review='llama-code -c 8192 -n 1024 --temp 0.1 --silent -p'
alias brainstorm='llama-creative -c 4096 -n 800 --temp 0.9 --top-p 0.95 -p'

Script de benchmark complet

#!/bin/bash
# Evaluar el rendimiento de todos tus modelos

benchmark_all() {
    local models_dir="$1"
    
    echo "üìä BENCHMARK DE MODELOS"
    echo "======================"
    
    for model in "$models_dir"/*.gguf; do
        model_name=$(basename "$model" .gguf)
        echo "üß™ Evaluando: $model_name"
        
        ./llama-bench \
            -m "$model" \
            -p 512 \
            -n 128 \
            -t $(nproc) 2>/dev/null | \
            grep "llama_print_timings" || echo "‚ùå Error en $model_name"
        echo ""
    done
    
    echo "‚úÖ Benchmark completado"
}

# Ejecutar benchmark
benchmark_all "./modelos"


‚∏ª

üìö R√©f√©rences techniques

Formats de quantification (tri√©s par qualit√©/taille)

Format	Qualit√©	Taille	Recommand√© pour
Q8_0	99%	50% de l‚Äôoriginal	Qualit√© maximale, hardware puissant
Q6_K	98%	60% de l‚Äôoriginal	Excellent √©quilibre
Q5_K_M	95%	70% de l‚Äôoriginal	Usage g√©n√©ral recommand√©
Q4_K_M	90%	50% de l‚Äôoriginal	Hardware limit√©
Q4_K_S	85%	45% de l‚Äôoriginal	Compression maximale utile

Commandes de diagnostic

# Verificar integridad de modelo
./llama-gguf-hash -f modelo.gguf

# An√°lisis de tokenizaci√≥n
./llama-tokenize -m modelo.gguf -p "Tu texto aqu√≠"

# Test r√°pido de funcionalidad
./llama-cli -m modelo.gguf -p "2+2=" -n 5 --temp 0.1

# Informaci√≥n del modelo
./llama-cli -m modelo.gguf --help | head -20

Prochaines √©tapes recommand√©es
	1.	Exp√©rimentez avec les temp√©ratures pour diff√©rents types de t√¢ches
	2.	Configurez des alias pour vos workflows les plus courants
	3.	Testez le mode serveur pour int√©grer √† d‚Äôautres applications
	4.	Optimisez le contexte selon le type de documents trait√©s
	5.	Automatisez la s√©lection des mod√®les selon le contenu

‚∏ª

Structure typique : ./llama.cpp/build/bin/ (binaires) et ./modelos/ (fichiers .gguf)

Installation : Compiler llama.cpp depuis le d√©p√¥t officiel GitHub

Vous avez tout ce qu‚Äôil faut pour ma√Ætriser votre √©cosyst√®me d‚ÄôIA local !

Eto Demerzel (Gustavo Silva Da Costa)
https://etodemerzel.gumroad.com
https://github.com/BiblioGalactic