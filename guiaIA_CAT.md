# Guia Completa de llama.cpp - La Teva IA Local en Terminal

> Una guia prÃ ctica per dominar l'ecosistema de models d'IA locals

---

## ðŸ§  Conceptes Fonamentals (Per comenÃ§ar bÃ©)

### QuÃ¨ Ã©s un arxiu `.gguf`?
Ã‰s com un **arxiu ZIP perÃ² per a models d'IA**. ContÃ© tot el necessari:
- L'arquitectura del model (com estÃ  construÃ¯t)
- Els pesos neuronals (el "coneixement" entrenat)
- El tokenitzador (com converteix text en nÃºmeros)
- Metadades i configuraciÃ³

**Per aixÃ² sÃ³n portÃ tils**: copies l'arxiu i ja tens la IA completa.

### QuÃ¨ Ã©s la QuantitzaciÃ³?
Imagina una foto en 4K vs la mateixa foto comprimida en JPEG:
- **F16**: PrecisiÃ³ mÃ xima, consum de RAM brutal
- **Q8_0**: GairebÃ© sense pÃ¨rdua, 50% menys RAM
- **Q6_K**: ExcelÂ·lent equilibri qualitat/mida
- **Q5_K_M**: Bona qualitat, compacte
- **Q4_K_M**: EstÃ ndard recomanat per a la majoria
- **Q4_K_S**: Molt compacte, pÃ¨rdua notable perÃ² funcional

### Tokens: La "Moneda" de la IA
Els models no llegeixen paraules, llegeixen **tokens**:
- `"Hola"` = 1 token
- `"artificial"` = 2 tokens (`"art"` + `"ificial"`)
- `"ðŸ¤–"` = 1 token

**Regla prÃ ctica**: 1 token â‰ˆ 0.75 paraules en catalÃ .

---

## ðŸŽ¯ Tipus de Models i les Seves Especialitats

### Per Idioma i Domini

| Especialitat | Idioma Principal | Estil | Casos d'Ãšs TÃ­pics |
|--------------|-----------------|-------|-------------------|
| **TÃ¨cnic/Alemany** | Alemany | Formal, normatiu | Documents oficials, traduccions precises |
| **ProgramaciÃ³** | AnglÃ¨s | AnÃ lisi estructurada | Debugging, arquitectura, code review |
| **JaponÃ¨s** | JaponÃ¨s | Imperatius, instruccions | DocumentaciÃ³ tÃ¨cnica japonesa |
| **Context Llarg** | MultilingÃ¼e | Coneixement ampli | AnÃ lisis complexes, recerca |
| **General/Balancejat** | AnglÃ¨s | GeneraciÃ³ fluida | Ãšs diari, tasques variades |
| **MÃ¨dic/ClÃ­nic** | AnglÃ¨s | ClÃ­nic, biomÃ¨dic | Textos mÃ¨dics, farmacÃ¨utics |
| **Conversacional** | AnglÃ¨s | DiÃ leg natural | Xat, atenciÃ³ al client |
| **DocumentaciÃ³** | AnglÃ¨s | Explicacions suaus | Manuals, guies tÃ¨cniques |
| **Compacte/RÃ pid** | AnglÃ¨s | Raonament eficient | Proves rÃ pides, maquinari limitat |
| **Multiidioma** | XinÃ¨s/AnglÃ¨s | Context extens | Documents internacionals |
| **Ultra Lleuger** | AnglÃ¨s | Testing rÃ pid | Desenvolupament, IoT, experiments |
| **Python Especialitzat** | AnglÃ¨s | Tutorials detallats | Ensenyament de programaciÃ³ |
| **AcadÃ¨mic** | XinÃ¨s/AnglÃ¨s | Papers cientÃ­fics | Recerca, textos tÃ¨cnics |
| **JurÃ­dic/Formal** | AnglÃ¨s | Institucional | Contractes, polÃ­tiques, compliance |
| **AutomatitzaciÃ³** | AnglÃ¨s | Decisions complexes | Workflows, gestiÃ³ de processos |
| **Base Sense Filtres** | AnglÃ¨s | Neutral | Experiments, respostes directes |
| **Narratiu** | AnglÃ¨s | MitolÃ²gic, storytelling | Worldbuilding, ficciÃ³ Ã¨pica |
| **Creatiu Expressiu** | AnglÃ¨s | DramÃ tic, emotiu | FicciÃ³, roleplay creatiu |
| **FilosÃ²fic** | AnglÃ¨s | DiÃ leg socrÃ tic | Debats, pensament crÃ­tic |
| **Sense Censura** | AnglÃ¨s | Temes sensibles | Recerca de seguretat |
| **Roleplay AvanÃ§at** | AnglÃ¨s | Narrativa lliure | Roleplay, exploraciÃ³ creativa |

---

## ðŸ›  Eines de l'Ecosistema

### 1. Nucli d'ExecuciÃ³

| Binari | FunciÃ³ Principal | Quan Usar |
|---------|-------------------|-------------|
| `llama-cli` | **Motor principal**. Executa models des de terminal | Scripts, automatitzaciÃ³, proves rÃ pides |
| `llama-run` | Xat interactiu amb memÃ²ria de conversa | Experimentar, dialogar amb models |
| `llama-server` | Servidor web amb API REST | Integrar amb aplicacions, Ãºs remot |

### 2. Eines d'AnÃ lisi

| Eina | FunciÃ³ | Utilitat PrÃ ctica |
|-------------|---------|-------------------|
| `llama-tokenize` | Mostra com el model interpreta el teu text | Optimitzar prompts, entendre lÃ­mits |
| `llama-bench` | Mesura rendiment al teu maquinari | Comparar models, optimitzar configuraciÃ³ |
| `llama-embedding` | Converteix text en vectors numÃ¨rics | Cerca semÃ ntica, anÃ lisi de similitud |

### 3. Eines d'OptimitzaciÃ³

| Eina | PropÃ²sit | Quan Ã©s NecessÃ ria |
|-------------|-----------|-------------------|
| `llama-quantize` | Comprimeix models per menys RAM | El teu maquinari no suporta el model complet |
| `llama-gguf-split` | Divideix models en fragments | DescÃ rregues lentes, emmagatzematge limitat |
| `llama-gguf-hash` | Verifica integritat d'arxius | Assegurar descÃ rregues correctes |

---

## âš™ï¸ ParÃ metres Essencials

### BÃ sics (Imprescindibles)

| ParÃ metre | FunciÃ³ | Valors TÃ­pics | Exemple PrÃ ctic |
|-----------|---------|----------------|------------------|
| `-m` | Ruta al model | Ruta absoluta | `-m ~/models/mistral-7b.gguf` |
| `-p` | El teu prompt/pregunta | Text lliure | `-p "Explica la fotosÃ­ntesi"` |
| `-n, --n-predict` | Tokens mÃ xims a generar | 128-2048 | `-n 512` (resposta mitjana) |
| `-c, --ctx-size` | Mida del context | 512-16384 | `-c 4096` (document llarg) |

### Control de Context i MemÃ²ria

| ConfiguraciÃ³ | Ãšs de RAM Aprox. | Escenari Ideal |
|---------------|-------------------|-----------------|
| `--ctx-size 1024` | ~1-2MB | Xat bÃ sic, preguntes curtes |
| `--ctx-size 2048` | ~2-4MB | Converses normals |
| `--ctx-size 4096` | ~4-8MB | Documents mitjans, anÃ lisi |
| `--ctx-size 8192` | ~8-16MB | Textos llargs, recerca |
| `--ctx-size 16384` | ~16-32MB | Documents molt extensos |

### Control de Creativitat

| Temperatura | Comportament | Casos d'Ãšs |
|-------------|----------------|--------------|
| `--temp 0.1` | **Robot**: Molt determinista | Codi, correccions, dades precises |
| `--temp 0.3` | **TÃ¨cnic**: PrecÃ­s perÃ² flexible | DocumentaciÃ³, explicacions |
| `--temp 0.7` | **HumÃ **: Equilibri natural | Conversa general |
| `--temp 0.9` | **Creatiu**: DinÃ mic | Brainstorming, idees |
| `--temp 1.2` | **Artista**: Molt experimental | FicciÃ³, narrativa lliure |

### Control de Qualitat de Sortida

| ParÃ metre | Efecte | Valor Conservador | Valor Creatiu |
|-----------|--------|------------------|----------------|
| `--top-p` | Varietat de vocabulari | 0.9 | 0.95 |
| `--top-k` | LÃ­mit d'opcions | 20-40 | 80-100 |
| `--repeat-penalty` | PrevÃ© repeticions | 1.1 | 1.05 |
| `--repeat-last-n` | Finestra anti-repeticiÃ³ | 64 | 128 |

### OptimitzaciÃ³ de Rendiment

| ParÃ metre | FunciÃ³ | ConfiguraciÃ³ TÃ­pica |
|-----------|---------|---------------------|
| `-t, --threads` | Fils de CPU | Nombre de nuclis disponibles |
| `--batch-size` | Processament per lots | 512-2048 (segons RAM) |
| `--gpu-layers` | Capes a GPU | 99 (usar tota la GPU disponible) |

---

## ðŸ“‹ Receptes per Especialitat

### ðŸ”§ CorrecciÃ³ i EdiciÃ³ de Text

```bash
# CorrecciÃ³ precisa de documents
./llama-cli \
    -m ./models/mistral-7b-instruct.gguf \
    -p "Corregeix errors ortogrÃ fics i gramaticals: $(cat document.txt)" \
    -c 4096 \
    -n 512 \
    --temp 0.2 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    --silent
```

### ðŸ’» AnÃ lisi i RevisiÃ³ de Codi

```bash
# RevisiÃ³ de codi amb anÃ lisi detallada
./llama-cli \
    -m ./models/deepseek-coder.gguf \
    -p "Analitza aquest codi i suggereix millores: $(cat script.py)" \
    -c 8192 \
    -n 1024 \
    --temp 0.1 \
    --repeat-penalty 1.1 \
    --silent
```

### ðŸŽ¨ GeneraciÃ³ Creativa

```bash
# Escriptura creativa amb alta expressivitat
./llama-cli \
    -m ./models/chronos-hermes.gguf \
    -p "Escriu una histÃ²ria Ã¨pica sobre el despertar d'una IA" \
    -c 4096 \
    -n 1500 \
    --temp 0.9 \
    --top-p 0.95 \
    --repeat-penalty 1.05
```

### ðŸ” AnÃ lisi de Documents Extensos

```bash
# Processament de context molt llarg
./llama-cli \
    -m ./models/llama-70b.gguf \
    -p "Resumeix i analitza aquest document complet: $(cat document_llarg.txt)" \
    -c 16384 \
    -n 2048 \
    --temp 0.5 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    -t 8
```

### ðŸ’¬ Conversa Natural

```bash
# Xat interactiu amb memÃ²ria
./llama-run \
    -m ./models/openchat.gguf \
    --repeat-penalty 1.1 \
    --temp 0.7 \
    -c 2048 \
    -i
```

---

## ðŸ§ª EstratÃ¨gies per Tipus de Tasca

### Tasques TÃ¨cniques i Factuals
```bash
# ConfiguraciÃ³ per a precisiÃ³ mÃ xima
--temp 0.1-0.3 --top-p 0.9 --repeat-penalty 1.1
# Models recomanats: Coder, Medical, Technical
```

### Conversa i Explicacions
```bash
# ConfiguraciÃ³ balancejada i natural
--temp 0.6-0.8 --top-p 0.9 --repeat-penalty 1.1 --repeat-last-n 64
# Models recomanats: Chat, General-purpose, Instruction-following
```

### Creativitat i Brainstorming
```bash
# ConfiguraciÃ³ per a mÃ xima expressivitat
--temp 0.8-1.2 --top-p 0.95 --repeat-penalty 1.05
# Models recomanats: Creative, Storytelling, Roleplay
```

### Recerca i AnÃ lisi
```bash
# ConfiguraciÃ³ per a profunditat analÃ­tica
--temp 0.3-0.5 --top-p 0.9 -c 8192+ --n-predict 1024+
# Models recomanats: Large context, Academic, Research-focused
```

---

## ðŸš€ AutomatitzaciÃ³ i IntegraciÃ³

### Script de SelecciÃ³ AutomÃ tica de Model

```bash
#!/bin/bash
# Selector intelÂ·ligent basat en tipus de tasca

select_model_by_task() {
    local task="$1"
    local base_path="./models"
    
    case "$task" in
        "code"|"programming")
            echo "$base_path/deepseek-coder.gguf"
            ;;
        "creative"|"story")
            echo "$base_path/chronos-hermes.gguf"
            ;;
        "medical"|"health")
            echo "$base_path/meditron.gguf"
            ;;
        "legal"|"formal")
            echo "$base_path/nous-hermes-legal.gguf"
            ;;
        "research"|"academic")
            echo "$base_path/llama-70b.gguf"
            ;;
        *)
            echo "$base_path/mistral-instruct.gguf"
            ;;
    esac
}

# Ãšs del selector
TASK_TYPE="$1"
MODEL=$(select_model_by_task "$TASK_TYPE")
PROMPT="$2"

./llama-cli -m "$MODEL" -p "$PROMPT" -c 4096 -n 512 --temp 0.7
```

### Servidor Multi-Model

```bash
#!/bin/bash
# LlanÃ§ar mÃºltiples models com a serveis

start_model_server() {
    local model_path="$1"
    local port="$2"
    local model_name="$3"
    
    ./llama-server \
        -m "$model_path" \
        --host 0.0.0.0 \
        --port "$port" \
        -c 4096 \
        --gpu-layers 99 &
    
    echo "âœ… $model_name servidor iniciat al port $port"
}

# Iniciar serveis especialitzats
start_model_server "./models/mistral-general.gguf" 8080 "General"
start_model_server "./models/deepseek-coder.gguf" 8081 "Codi"
start_model_server "./models/creative-model.gguf" 8082 "Creatiu"

echo "ðŸŒ Servidors disponibles:"
echo "  General: http://localhost:8080"
echo "  Codi: http://localhost:8081"
echo "  Creatiu: http://localhost:8082"
```

### Pipeline de Processament de Documents

```bash
#!/bin/bash
# Pipeline complet: OCR â†’ CorrecciÃ³ â†’ AnÃ lisi

process_document() {
    local input_image="$1"
    local output_dir="./processed"
    
    mkdir -p "$output_dir"
    
    # 1. OCR del document
    tesseract "$input_image" "$output_dir/raw_text"
    
    # 2. CorrecciÃ³ amb IA
    ./llama-cli \
        -m ./models/correction-model.gguf \
        -p "Corregeix errors en aquest text: $(cat "$output_dir/raw_text.txt")" \
        -c 4096 -n 1024 --temp 0.2 --silent \
        > "$output_dir/corrected_text.txt"
    
    # 3. AnÃ lisi i resum
    ./llama-cli \
        -m ./models/analysis-model.gguf \
        -p "Resumeix els punts clau: $(cat "$output_dir/corrected_text.txt")" \
        -c 2048 -n 256 --temp 0.5 --silent \
        > "$output_dir/summary.txt"
    
    echo "âœ… Document processat a $output_dir"
}

# Usar el pipeline
process_document "document_escanejat.png"
```

---

## ðŸ›¡ SoluciÃ³ de Problemes Comuns

### Models Molt Grans (70B+)
**SÃ­mptoma**: Sistema lent o sense memÃ²ria suficient
**Solucions**:
```bash
# Reduir Ãºs de memÃ²ria
-c 2048              # Menys context
-t 4                 # Menys fils
--gpu-layers 50      # NomÃ©s part a GPU
# O usar quantitzaciÃ³ mÃ©s agressiva (Q4_K_S)
```

### Models de Codi
**SÃ­mptoma**: Respostes incompletes o codi tallat
**Solucions**:
```bash
-n 2048              # MÃ©s tokens de sortida
--temp 0.1           # MÃ xima precisiÃ³
-c 8192              # MÃ©s context per a codi llarg
--ignore-eos         # No tallar prematurament
```

### Models Creatius
**SÃ­mptoma**: RepeticiÃ³ o pÃ¨rdua de coherÃ¨ncia
**Solucions**:
```bash
--repeat-penalty 1.1  # Penalitzar repeticions
--mirostat 2          # Control automÃ tic
--temp 0.8            # No excedir temperatura
-c 4096+              # MÃ©s context per a coherÃ¨ncia
```

### DetecciÃ³ de Problemes de Rendiment
```bash
# Monitoritzar Ãºs de recursos
watch -n 1 'ps aux | grep llama-cli'

# Benchmark rÃ pid
./llama-bench -m model.gguf -p 512 -n 128

# Test de memÃ²ria
./llama-cli -m model.gguf -c 1024 -n 10 --temp 0.1 -p "Test"
```

---

## ðŸŽ¯ ConfiguraciÃ³ Productiva

### Variables d'Entorn Ãštils

```bash
# Afegir al teu .bashrc o .zshrc
export LLAMA_HOME="./llama.cpp/build/bin"
export MODELS_DIR="./models"

# Ã€lies per a Ãºs rÃ pid
alias llama='$LLAMA_HOME/llama-cli -m $MODELS_DIR/general-model.gguf'
alias llama-code='$LLAMA_HOME/llama-cli -m $MODELS_DIR/code-model.gguf'
alias llama-creative='$LLAMA_HOME/llama-cli -m $MODELS_DIR/creative-model.gguf'

# Configuracions predefinides
alias quick-fix='llama -n 256 --temp 0.2 --repeat-penalty 1.1 --silent -p'
alias code-review='llama-code -c 8192 -n 1024 --temp 0.1 --silent -p'
alias brainstorm='llama-creative -c 4096 -n 800 --temp 0.9 --top-p 0.95 -p'
```

### Script de Benchmark Complet

```bash
#!/bin/bash
# Avaluar el rendiment de tots els teus models

benchmark_all() {
    local models_dir="$1"
    
    echo "ðŸ“Š BENCHMARK DE MODELS"
    echo "======================"
    
    for model in "$models_dir"/*.gguf; do
        model_name=$(basename "$model" .gguf)
        echo "ðŸ§ª Avaluant: $model_name"
        
        ./llama-bench \
            -m "$model" \
            -p 512 \
            -n 128 \
            -t $(nproc) 2>/dev/null | \
            grep "llama_print_timings" || echo "âŒ Error en $model_name"
        echo ""
    done
    
    echo "âœ… Benchmark completat"
}

# Executar benchmark
benchmark_all "./models"
```

---

## ðŸ“š ReferÃ¨ncies TÃ¨cniques

### Formats de QuantitzaciÃ³ (Ordenats per Qualitat/Mida)

| Format | Qualitat | Mida | Recomanat Per |
|---------|---------|--------|------------------|
| **Q8_0** | 99% | 50% de l'original | MÃ xima qualitat, maquinari potent |
| **Q6_K** | 98% | 60% de l'original | Equilibri excelÂ·lent |
| **Q5_K_M** | 95% | 70% de l'original | Ãšs general recomanat |
| **Q4_K_M** | 90% | 50% de l'original | Maquinari limitat |
| **Q4_K_S** | 85% | 45% de l'original | MÃ xima compressiÃ³ Ãºtil |

### Comandaments de DiagnÃ²stic

```bash
# Verificar integritat del model
./llama-gguf-hash -f model.gguf

# AnÃ lisi de tokenitzaciÃ³
./llama-tokenize -m model.gguf -p "El teu text aquÃ­"

# Test rÃ pid de funcionalitat
./llama-cli -m model.gguf -p "2+2=" -n 5 --temp 0.1

# InformaciÃ³ del model
./llama-cli -m model.gguf --help | head -20
```

### Propers Passos Recomanats

1. **Experimenta amb temperatures** per a diferents tipus de tasques
2. **Configura Ã lies** per als teus workflows mÃ©s comuns
3. **Prova el mode servidor** per integrar amb altres aplicacions
4. **Optimitza el context** segons el tipus de documents que processis
5. **Automatitza la selecciÃ³** de models segons el contingut

---

> **Estructura tÃ­pica**: `./llama.cpp/build/bin/` (binaris) i `./models/` (arxius .gguf)
> 
> **InstalÂ·laciÃ³**: Compilar llama.cpp des del repositori oficial de GitHub

Tens tot el necessari per dominar el teu ecosistema d'IA local!

**Eto Demerzel** (Gustavo Silva Da Costa)
https://etodemerzel.gumroad.com  
https://github.com/BiblioGalactic