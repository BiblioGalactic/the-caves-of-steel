# Guia Completa de llama.cpp - La Teva IA Local en Terminal

> Una guia pr√†ctica per dominar l'ecosistema de models d'IA locals

---

## üß† Conceptes Fonamentals (Per comen√ßar b√©)

### Qu√® √©s un arxiu `.gguf`?
√âs com un **arxiu ZIP per√≤ per a models d'IA**. Cont√© tot el necessari:
- L'arquitectura del model (com est√† constru√Øt)
- Els pesos neuronals (el "coneixement" entrenat)
- El tokenitzador (com converteix text en n√∫meros)
- Metadades i configuraci√≥

**Per aix√≤ s√≥n port√†tils**: copies l'arxiu i ja tens la IA completa.

### Qu√® √©s la Quantitzaci√≥?
Imagina una foto en 4K vs la mateixa foto comprimida en JPEG:
- **F16**: Precisi√≥ m√†xima, consum de RAM brutal
- **Q8_0**: Gaireb√© sense p√®rdua, 50% menys RAM
- **Q6_K**: Excel¬∑lent equilibri qualitat/mida
- **Q5_K_M**: Bona qualitat, compacte
- **Q4_K_M**: Est√†ndard recomanat per a la majoria
- **Q4_K_S**: Molt compacte, p√®rdua notable per√≤ funcional

### Tokens: La "Moneda" de la IA
Els models no llegeixen paraules, llegeixen **tokens**:
- `"Hola"` = 1 token
- `"artificial"` = 2 tokens (`"art"` + `"ificial"`)
- `"ü§ñ"` = 1 token

**Regla pr√†ctica**: 1 token ‚âà 0.75 paraules en catal√†.

---

## üéØ Tipus de Models i les Seves Especialitats

### Per Idioma i Domini

| Especialitat | Idioma Principal | Estil | Casos d'√ös T√≠pics |
|--------------|-----------------|-------|-------------------|
| **T√®cnic/Alemany** | Alemany | Formal, normatiu | Documents oficials, traduccions precises |
| **Programaci√≥** | Angl√®s | An√†lisi estructurada | Debugging, arquitectura, code review |
| **Japon√®s** | Japon√®s | Imperatius, instruccions | Documentaci√≥ t√®cnica japonesa |
| **Context Llarg** | Multiling√ºe | Coneixement ampli | An√†lisis complexes, recerca |
| **General/Balancejat** | Angl√®s | Generaci√≥ fluida | √ös diari, tasques variades |
| **M√®dic/Cl√≠nic** | Angl√®s | Cl√≠nic, biom√®dic | Textos m√®dics, farmac√®utics |
| **Conversacional** | Angl√®s | Di√†leg natural | Xat, atenci√≥ al client |
| **Documentaci√≥** | Angl√®s | Explicacions suaus | Manuals, guies t√®cniques |
| **Compacte/R√†pid** | Angl√®s | Raonament eficient | Proves r√†pides, maquinari limitat |
| **Multiidioma** | Xin√®s/Angl√®s | Context extens | Documents internacionals |
| **Ultra Lleuger** | Angl√®s | Testing r√†pid | Desenvolupament, IoT, experiments |
| **Python Especialitzat** | Angl√®s | Tutorials detallats | Ensenyament de programaci√≥ |
| **Acad√®mic** | Xin√®s/Angl√®s | Papers cient√≠fics | Recerca, textos t√®cnics |
| **Jur√≠dic/Formal** | Angl√®s | Institucional | Contractes, pol√≠tiques, compliance |
| **Automatitzaci√≥** | Angl√®s | Decisions complexes | Workflows, gesti√≥ de processos |
| **Base Sense Filtres** | Angl√®s | Neutral | Experiments, respostes directes |
| **Narratiu** | Angl√®s | Mitol√≤gic, storytelling | Worldbuilding, ficci√≥ √®pica |
| **Creatiu Expressiu** | Angl√®s | Dram√†tic, emotiu | Ficci√≥, roleplay creatiu |
| **Filos√≤fic** | Angl√®s | Di√†leg socr√†tic | Debats, pensament cr√≠tic |
| **Sense Censura** | Angl√®s | Temes sensibles | Recerca de seguretat |
| **Roleplay Avan√ßat** | Angl√®s | Narrativa lliure | Roleplay, exploraci√≥ creativa |

---

## üõ† Eines de l'Ecosistema

### 1. Nucli d'Execuci√≥

| Binari | Funci√≥ Principal | Quan Usar |
|---------|-------------------|-------------|
| `llama-cli` | **Motor principal**. Executa models des de terminal | Scripts, automatitzaci√≥, proves r√†pides |
| `llama-run` | Xat interactiu amb mem√≤ria de conversa | Experimentar, dialogar amb models |
| `llama-server` | Servidor web amb API REST | Integrar amb aplicacions, √∫s remot |

### 2. Eines d'An√†lisi

| Eina | Funci√≥ | Utilitat Pr√†ctica |
|-------------|---------|-------------------|
| `llama-tokenize` | Mostra com el model interpreta el teu text | Optimitzar prompts, entendre l√≠mits |
| `llama-bench` | Mesura rendiment al teu maquinari | Comparar models, optimitzar configuraci√≥ |
| `llama-embedding` | Converteix text en vectors num√®rics | Cerca sem√†ntica, an√†lisi de similitud |

### 3. Eines d'Optimitzaci√≥

| Eina | Prop√≤sit | Quan √©s Necess√†ria |
|-------------|-----------|-------------------|
| `llama-quantize` | Comprimeix models per menys RAM | El teu maquinari no suporta el model complet |
| `llama-gguf-split` | Divideix models en fragments | Desc√†rregues lentes, emmagatzematge limitat |
| `llama-gguf-hash` | Verifica integritat d'arxius | Assegurar desc√†rregues correctes |

---

## ‚öôÔ∏è Par√†metres Essencials

### B√†sics (Imprescindibles)

| Par√†metre | Funci√≥ | Valors T√≠pics | Exemple Pr√†ctic |
|-----------|---------|----------------|------------------|
| `-m` | Ruta al model | Ruta absoluta | `-m ~/models/mistral-7b.gguf` |
| `-p` | El teu prompt/pregunta | Text lliure | `-p "Explica la fotos√≠ntesi"` |
| `-n, --n-predict` | Tokens m√†xims a generar | 128-2048 | `-n 512` (resposta mitjana) |
| `-c, --ctx-size` | Mida del context | 512-16384 | `-c 4096` (document llarg) |

### Control de Context i Mem√≤ria

| Configuraci√≥ | √ös de RAM Aprox. | Escenari Ideal |
|---------------|-------------------|-----------------|
| `--ctx-size 1024` | ~1-2MB | Xat b√†sic, preguntes curtes |
| `--ctx-size 2048` | ~2-4MB | Converses normals |
| `--ctx-size 4096` | ~4-8MB | Documents mitjans, an√†lisi |
| `--ctx-size 8192` | ~8-16MB | Textos llargs, recerca |
| `--ctx-size 16384` | ~16-32MB | Documents molt extensos |

### Control de Creativitat

| Temperatura | Comportament | Casos d'√ös |
|-------------|----------------|--------------|
| `--temp 0.1` | **Robot**: Molt determinista | Codi, correccions, dades precises |
| `--temp 0.3` | **T√®cnic**: Prec√≠s per√≤ flexible | Documentaci√≥, explicacions |
| `--temp 0.7` | **Hum√†**: Equilibri natural | Conversa general |
| `--temp 0.9` | **Creatiu**: Din√†mic | Brainstorming, idees |
| `--temp 1.2` | **Artista**: Molt experimental | Ficci√≥, narrativa lliure |

### Control de Qualitat de Sortida

| Par√†metre | Efecte | Valor Conservador | Valor Creatiu |
|-----------|--------|------------------|----------------|
| `--top-p` | Varietat de vocabulari | 0.9 | 0.95 |
| `--top-k` | L√≠mit d'opcions | 20-40 | 80-100 |
| `--repeat-penalty` | Prev√© repeticions | 1.1 | 1.05 |
| `--repeat-last-n` | Finestra anti-repetici√≥ | 64 | 128 |

### Optimitzaci√≥ de Rendiment

| Par√†metre | Funci√≥ | Configuraci√≥ T√≠pica |
|-----------|---------|---------------------|
| `-t, --threads` | Fils de CPU | Nombre de nuclis disponibles |
| `--batch-size` | Processament per lots | 512-2048 (segons RAM) |
| `--gpu-layers` | Capes a GPU | 99 (usar tota la GPU disponible) |

---

## üìã Receptes per Especialitat

### üîß Correcci√≥ i Edici√≥ de Text

```bash
# Correcci√≥ precisa de documents
./llama-cli \
    -m ./models/mistral-7b-instruct.gguf \
    -p "Corregeix errors ortogr√†fics i gramaticals: $(cat document.txt)" \
    -c 4096 \
    -n 512 \
    --temp 0.2 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    --silent
```

### üíª An√†lisi i Revisi√≥ de Codi

```bash
# Revisi√≥ de codi amb an√†lisi detallada
./llama-cli \
    -m ./models/deepseek-coder.gguf \
    -p "Analitza aquest codi i suggereix millores: $(cat script.py)" \
    -c 8192 \
    -n 1024 \
    --temp 0.1 \
    --repeat-penalty 1.1 \
    --silent
```

### üé® Generaci√≥ Creativa

```bash
# Escriptura creativa amb alta expressivitat
./llama-cli \
    -m ./models/chronos-hermes.gguf \
    -p "Escriu una hist√≤ria √®pica sobre el despertar d'una IA" \
    -c 4096 \
    -n 1500 \
    --temp 0.9 \
    --top-p 0.95 \
    --repeat-penalty 1.05
```

### üîç An√†lisi de Documents Extensos

```bash
# Processament de context molt llarg
./llama-cli \
    -m ./models/llama-70b.gguf \
    -p "Resumeix i analitza aquest document complet: $(cat document_llarg.txt)" \
    -c 16384 \
    -n 2048 \
    --temp 0.5 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    -t 8
```

### üí¨ Conversa Natural

```bash
# Xat interactiu amb mem√≤ria
./llama-run \
    -m ./models/openchat.gguf \
    --repeat-penalty 1.1 \
    --temp 0.7 \
    -c 2048 \
    -i
```

---

## üß™ Estrat√®gies per Tipus de Tasca

### Tasques T√®cniques i Factuals
```bash
# Configuraci√≥ per a precisi√≥ m√†xima
--temp 0.1-0.3 --top-p 0.9 --repeat-penalty 1.1
# Models recomanats: Coder, Medical, Technical
```

### Conversa i Explicacions
```bash
# Configuraci√≥ balancejada i natural
--temp 0.6-0.8 --top-p 0.9 --repeat-penalty 1.1 --repeat-last-n 64
# Models recomanats: Chat, General-purpose, Instruction-following
```

### Creativitat i Brainstorming
```bash
# Configuraci√≥ per a m√†xima expressivitat
--temp 0.8-1.2 --top-p 0.95 --repeat-penalty 1.05
# Models recomanats: Creative, Storytelling, Roleplay
```

### Recerca i An√†lisi
```bash
# Configuraci√≥ per a profunditat anal√≠tica
--temp 0.3-0.5 --top-p 0.9 -c 8192+ --n-predict 1024+
# Models recomanats: Large context, Academic, Research-focused
```

---

## üöÄ Automatitzaci√≥ i Integraci√≥

### Script de Selecci√≥ Autom√†tica de Model

```bash
#!/bin/bash
# Selector intel¬∑ligent basat en tipus de tasca

select_model_by_task() {
    local task="$1"
    local base_path="./models"
    
    case "$task" in
        "code"|"programming")
            echo "$base_path/deepseek-coder.gguf"
            ;;
        "creative"|"story")
            echo "$base_path/chronos-hermes.gguf"
            ;;
        "medical"|"health")
            echo "$base_path/meditron.gguf"
            ;;
        "legal"|"formal")
            echo "$base_path/nous-hermes-legal.gguf"
            ;;
        "research"|"academic")
            echo "$base_path/llama-70b.gguf"
            ;;
        *)
            echo "$base_path/mistral-instruct.gguf"
            ;;
    esac
}

# √ös del selector
TASK_TYPE="$1"
MODEL=$(select_model_by_task "$TASK_TYPE")
PROMPT="$2"

./llama-cli -m "$MODEL" -p "$PROMPT" -c 4096 -n 512 --temp 0.7
```

### Servidor Multi-Model

```bash
#!/bin/bash
# Llan√ßar m√∫ltiples models com a serveis

start_model_server() {
    local model_path="$1"
    local port="$2"
    local model_name="$3"
    
    ./llama-server \
        -m "$model_path" \
        --host 0.0.0.0 \
        --port "$port" \
        -c 4096 \
        --gpu-layers 99 &
    
    echo "‚úÖ $model_name servidor iniciat al port $port"
}

# Iniciar serveis especialitzats
start_model_server "./models/mistral-general.gguf" 8080 "General"
start_model_server "./models/deepseek-coder.gguf" 8081 "Codi"
start_model_server "./models/creative-model.gguf" 8082 "Creatiu"

echo "üåê Servidors disponibles:"
echo "  General: http://localhost:8080"
echo "  Codi: http://localhost:8081"
echo "  Creatiu: http://localhost:8082"
```

### Pipeline de Processament de Documents

```bash
#!/bin/bash
# Pipeline complet: OCR ‚Üí Correcci√≥ ‚Üí An√†lisi

process_document() {
    local input_image="$1"
    local output_dir="./processed"
    
    mkdir -p "$output_dir"
    
    # 1. OCR del document
    tesseract "$input_image" "$output_dir/raw_text"
    
    # 2. Correcci√≥ amb IA
    ./llama-cli \
        -m ./models/correction-model.gguf \
        -p "Corregeix errors en aquest text: $(cat "$output_dir/raw_text.txt")" \
        -c 4096 -n 1024 --temp 0.2 --silent \
        > "$output_dir/corrected_text.txt"
    
    # 3. An√†lisi i resum
    ./llama-cli \
        -m ./models/analysis-model.gguf \
        -p "Resumeix els punts clau: $(cat "$output_dir/corrected_text.txt")" \
        -c 2048 -n 256 --temp 0.5 --silent \
        > "$output_dir/summary.txt"
    
    echo "‚úÖ Document processat a $output_dir"
}

# Usar el pipeline
process_document "document_escanejat.png"
```

---

## üõ° Soluci√≥ de Problemes Comuns

### Models Molt Grans (70B+)
**S√≠mptoma**: Sistema lent o sense mem√≤ria suficient
**Solucions**:
```bash
# Reduir √∫s de mem√≤ria
-c 2048              # Menys context
-t 4                 # Menys fils
--gpu-layers 50      # Nom√©s part a GPU
# O usar quantitzaci√≥ m√©s agressiva (Q4_K_S)
```

### Models de Codi
**S√≠mptoma**: Respostes incompletes o codi tallat
**Solucions**:
```bash
-n 2048              # M√©s tokens de sortida
--temp 0.1           # M√†xima precisi√≥
-c 8192              # M√©s context per a codi llarg
--ignore-eos         # No tallar prematurament
```

### Models Creatius
**S√≠mptoma**: Repetici√≥ o p√®rdua de coher√®ncia
**Solucions**:
```bash
--repeat-penalty 1.1  # Penalitzar repeticions
--mirostat 2          # Control autom√†tic
--temp 0.8            # No excedir temperatura
-c 4096+              # M√©s context per a coher√®ncia
```

### Detecci√≥ de Problemes de Rendiment
```bash
# Monitoritzar √∫s de recursos
watch -n 1 'ps aux | grep llama-cli'

# Benchmark r√†pid
./llama-bench -m model.gguf -p 512 -n 128

# Test de mem√≤ria
./llama-cli -m model.gguf -c 1024 -n 10 --temp 0.1 -p "Test"
```

---

## üéØ Configuraci√≥ Productiva

### Variables d'Entorn √ötils

```bash
# Afegir al teu .bashrc o .zshrc
export LLAMA_HOME="./llama.cpp/build/bin"
export MODELS_DIR="./models"

# √Älies per a √∫s r√†pid
alias llama='$LLAMA_HOME/llama-cli -m $MODELS_DIR/general-model.gguf'
alias llama-code='$LLAMA_HOME/llama-cli -m $MODELS_DIR/code-model.gguf'
alias llama-creative='$LLAMA_HOME/llama-cli -m $MODELS_DIR/creative-model.gguf'

# Configuracions predefinides
alias quick-fix='llama -n 256 --temp 0.2 --repeat-penalty 1.1 --silent -p'
alias code-review='llama-code -c 8192 -n 1024 --temp 0.1 --silent -p'
alias brainstorm='llama-creative -c 4096 -n 800 --temp 0.9 --top-p 0.95 -p'
```

### Script de Benchmark Complet

```bash
#!/bin/bash
# Avaluar el rendiment de tots els teus models

benchmark_all() {
    local models_dir="$1"
    
    echo "üìä BENCHMARK DE MODELS"
    echo "======================"
    
    for model in "$models_dir"/*.gguf; do
        model_name=$(basename "$model" .gguf)
        echo "üß™ Avaluant: $model_name"
        
        ./llama-bench \
            -m "$model" \
            -p 512 \
            -n 128 \
            -t $(nproc) 2>/dev/null | \
            grep "llama_print_timings" || echo "‚ùå Error en $model_name"
        echo ""
    done
    
    echo "‚úÖ Benchmark completat"
}

# Executar benchmark
benchmark_all "./models"
```

---

## üìö Refer√®ncies T√®cniques

### Formats de Quantitzaci√≥ (Ordenats per Qualitat/Mida)

| Format | Qualitat | Mida | Recomanat Per |
|---------|---------|--------|------------------|
| **Q8_0** | 99% | 50% de l'original | M√†xima qualitat, maquinari potent |
| **Q6_K** | 98% | 60% de l'original | Equilibri excel¬∑lent |
| **Q5_K_M** | 95% | 70% de l'original | √ös general recomanat |
| **Q4_K_M** | 90% | 50% de l'original | Maquinari limitat |
| **Q4_K_S** | 85% | 45% de l'original | M√†xima compressi√≥ √∫til |

### Comandaments de Diagn√≤stic

```bash
# Verificar integritat del model
./llama-gguf-hash -f model.gguf

# An√†lisi de tokenitzaci√≥
./llama-tokenize -m model.gguf -p "El teu text aqu√≠"

# Test r√†pid de funcionalitat
./llama-cli -m model.gguf -p "2+2=" -n 5 --temp 0.1

# Informaci√≥ del model
./llama-cli -m model.gguf --help | head -20
```

### Propers Passos Recomanats

1. **Experimenta amb temperatures** per a diferents tipus de tasques
2. **Configura √†lies** per als teus workflows m√©s comuns
3. **Prova el mode servidor** per integrar amb altres aplicacions
4. **Optimitza el context** segons el tipus de documents que processis
5. **Automatitza la selecci√≥** de models segons el contingut

---

> **Estructura t√≠pica**: `./llama.cpp/build/bin/` (binaris) i `./models/` (arxius .gguf)
> 
> **Instal¬∑laci√≥**: Compilar llama.cpp des del repositori oficial de GitHub

Tens tot el necessari per dominar el teu ecosistema d'IA local!

**Eto Demerzel** (Gustavo Silva Da Costa)
https://etodemerzel.gumroad.com  
https://github.com/BiblioGalactic